# -*- coding: utf-8 -*-
"""movie_dataviz.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5BNb_s0ZBzx2LeYgRaVvPdQ30LiEVIk
"""

# Importation des librairies nécessaires
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
from moviesdk import MovieClient, MovieConfig
import time
import json
from collections import Counter, defaultdict
from pathlib import Path

# Dossiers
output_dir = Path("output")
output_dir.mkdir(exist_ok=True)

# Connexion à l'API via le SDK

config = MovieConfig(movie_base_url="https://movielens-api-rmr7.onrender.com")
client = MovieClient(config=config)

# Vérification que l'API est opérationnelle
client.health_check()

# Récupération des stats de l'API
analytics = client.get_analytics()
analytics

"""## Top 10 des genres par nombre de films"""

'''
########### Sans système de mise en cache #########

# Initialisation du compteur de genres
genre_counter = Counter()

# Paramètres pour le batching
limit = 500
skip = 0

while True:
    batch = client.list_movies(skip=skip, limit=limit, output_format="dict")
    if not batch:
        break

    # On extrait les genres du lot et les compte
    for movie in batch:
        genres = movie.get("genres", "")
        genre_list = genres.split("|") if genres else []
        genre_counter.update(genre_list)

    skip += limit
    time.sleep(0.5)  # Pour respecter l’API

# Conversion du Counter en DataFrame
genre_df = pd.DataFrame(genre_counter.items(), columns=["genre", "count"])
genre_df = genre_df.sort_values("count", ascending=False).head(10)


# Bar chart horizontal
fig = px.bar(
    genre_df,
    x="count",
    y="genre",
    title="Top 10 genres par nombre de films",
    labels={"genre": "Genre", "count": "Nombre de films"},
    color="count",
    color_continuous_scale="viridis",
    orientation='h'  # ← clé pour l'affichage horizontal
)

fig.update_layout(
    yaxis={'categoryorder':'total ascending'},  # trie du haut vers le bas
    height=500
)

fig.show()

'''
0

# pip install pyarrow (a installer via le terminal)

####### Avec Système de mise en cache #########
api_movie_count = analytics.movie_count
print(api_movie_count)

genre_data_file = output_dir / "genre_df.parquet"
meta_file = output_dir / "meta.json"

# Lecture du fichier méta s'il existe
if meta_file.exists():
    with open(meta_file, "r") as f:
        meta = json.load(f)
    cached_movie_count = meta.get("movie_count", 0)
else:
    meta = {}
    cached_movie_count = 0

# Décision : utiliser le cache ou recalculer
if genre_data_file.exists() and cached_movie_count == api_movie_count:
    print("Chargement des données depuis le cache...")
    genre_df = pd.read_parquet(genre_data_file)
else:
    print("Mise à jour des données depuis l'API...")
    # Initialisation du compteur de genres
    genre_counter = Counter()

    # Paramètres pour le batching
    limit = 500
    skip = 0

    while True:
        batch = client.list_movies(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break

        # On extrait les genres du lot et les compte
        for movie in batch:
            genres = movie.get("genres", "")
            genre_list = genres.split("|") if genres else []
            genre_counter.update(genre_list)

        skip += limit
        time.sleep(0.5)  # Pour respecter l’API

    # Conversion du Counter en DataFrame
    genre_df = pd.DataFrame(genre_counter.items(), columns=["genre", "count"])
    genre_df = genre_df.sort_values("count", ascending=False).head(10)

    # Sauvegarde
    genre_df.to_parquet(genre_data_file, index=False)
    with open(meta_file, "w") as f:
        json.dump({"movie_count": api_movie_count}, f)

# Affichage Plotly
fig = px.bar(
    genre_df,
    x="count",
    y="genre",
    title="Top 10 genres par nombre de films",
    labels={"genre": "Genre", "count": "Nombre de films"},
    color="count",
    color_continuous_scale="viridis",
    orientation='h'
)

fig.update_layout(
    yaxis={'categoryorder':'total ascending'},
    height=500
)

fig.show()

genre_df

"""## Top 10 genres par nombre d’évaluations et note moyenne"""

'''
######## Sans mise en cache ###########
import plotly.express as px
from collections import defaultdict

# Initialisation des compteurs
genre_rating_count = defaultdict(int)     # total de ratings par genre
genre_rating_sum = defaultdict(float)     # somme des notes par genre

# Paramètres pour le batching
limit = 500
skip = 0

# Dictionnaire pour stocker les genres des films (évite de redemander plusieurs fois)
movie_genres = {}

while True:
    # Récupération d'un lot de ratings
    ratings_batch = client.list_ratings(skip=skip, limit=limit, output_format="dict")
    if not ratings_batch:
        break

    # Pour chaque évaluation dans le lot
    for rating in ratings_batch:
        movie_id = rating["movieId"]
        score = rating["rating"]

        # Si on n’a pas encore les genres de ce film, on va les chercher via l’API
        if movie_id not in movie_genres:
            try:
                # # Tentative de récupération des données du film
                movie_data = client.get_movie(movie_id)
                genres = movie_data.genres.split("|") if movie_data.genres else []
                movie_genres[movie_id] = genres
            except Exception as e:
                # Si une erreur survient (par exemple, un film inexistant), on la log et on passe au suivant
                print(f"Erreur lors de la récupération des données pour movieId {movie_id}: {e}")
                continue  # Ignore ce film et passe au suivant

        # Incrémenter les compteurs pour chaque genre du film
        for genre in movie_genres[movie_id]:
            genre_rating_count[genre] += 1
            genre_rating_sum[genre] += score

    skip += limit
    time.sleep(0.5)  # Respecter l’API

# Construction d’un DataFrame avec les résultats
genre_stats = pd.DataFrame([
    {
        "genre": genre,
        "rating_count": genre_rating_count[genre],
        "avg_rating": genre_rating_sum[genre] / genre_rating_count[genre]
    }
    for genre in genre_rating_count
])

# Sélection du Top 10 genres par nombre d’évaluations
top10_genre_stats = genre_stats.sort_values("rating_count", ascending=False).head(10)

# Affichage du barplot horizontal avec Plotly
fig = px.bar(
    top10_genre_stats,
    x="rating_count",
    y="genre",
    orientation="h",
    color="avg_rating",
    color_continuous_scale="viridis",
    title="Top 10 genres par nombre d’évaluations et note moyenne",
    labels={"genre": "Genre", "rating_count": "Nombre d'évaluations", "avg_rating": "Note moyenne"}
)

fig.update_layout(
    yaxis={'categoryorder':'total ascending'},
    height=500
)

fig.show()
'''
0

#top10_genre_stats

#### Version avec Système de mise en cache ####

# === Initialisation des dossiers ===
#output_dir = Path("output")
#output_dir.mkdir(exist_ok=True)

genre_data_file = output_dir / "genre_rating_stats.parquet"
meta_file = output_dir / "meta_genre_rating_stats.json"

# === Récupération des statistiques globales de l'API ===
#analytics = client.get_analytics()
api_rating_count = analytics.rating_count
api_movie_count = analytics.movie_count

# === Lecture du cache s'il existe ===
if meta_file.exists():
    with open(meta_file, "r") as f:
        meta = json.load(f)
    cached_rating_count = meta.get("rating_count", 0)
    cached_movie_count = meta.get("movie_count", 0)
else:
    cached_rating_count = 0
    cached_movie_count = 0

# === Décision : utiliser le cache ou recalculer ===
if genre_data_file.exists() and \
   cached_rating_count == api_rating_count and \
   cached_movie_count == api_movie_count:

    print("Chargement des données depuis le cache...")
    genre_stats = pd.read_parquet(genre_data_file)

else:
    print("Mise à jour des données depuis l'API...")

    # === Initialisation des compteurs ===
    genre_rating_count = defaultdict(int)
    genre_rating_sum = defaultdict(float)
    movie_genres = {}

    # === Paramètres pour le batching ===
    limit = 500
    skip = 0

    while True:
        ratings_batch = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not ratings_batch:
            break

        for rating in ratings_batch:
            movie_id = rating["movieId"]
            score = rating["rating"]

            # Récupération des genres si nécessaire
            if movie_id not in movie_genres:
                try:
                    movie_data = client.get_movie(movie_id)
                    genres = movie_data.genres.split("|") if movie_data.genres else []
                    movie_genres[movie_id] = genres
                except Exception as e:
                    print(f"Erreur lors de la récupération de movieId {movie_id}: {e}")
                    continue

            # Mise à jour des compteurs pour chaque genre
            for genre in movie_genres[movie_id]:
                genre_rating_count[genre] += 1
                genre_rating_sum[genre] += score

        skip += limit
        time.sleep(0.5)  # Respect de l'API

    # === Construction du DataFrame ===
    genre_stats = pd.DataFrame([
        {
            "genre": genre,
            "rating_count": genre_rating_count[genre],
            "avg_rating": genre_rating_sum[genre] / genre_rating_count[genre]
        }
        for genre in genre_rating_count
    ])

    # === Sauvegarde du cache ===
    genre_stats.to_parquet(genre_data_file, index=False)
    with open(meta_file, "w") as f:
        json.dump({
            "rating_count": api_rating_count,
            "movie_count": api_movie_count
        }, f)

# === Sélection du Top 10 et affichage ===
top10_genre_stats = genre_stats.sort_values("rating_count", ascending=False).head(10)

fig = px.bar(
    top10_genre_stats,
    x="rating_count",
    y="genre",
    orientation="h",
    color="avg_rating",
    color_continuous_scale="viridis",
    title="Top 10 genres par nombre d’évaluations et note moyenne",
    labels={"genre": "Genre", "rating_count": "Nombre d'évaluations", "avg_rating": "Note moyenne"}
)

fig.update_layout(
    yaxis={'categoryorder': 'total ascending'},
    height=500
)

fig.show()

"""## Nombre total de films par année (basé sur le titre)"""

import re

# === Dossiers ===
#output_dir = Path("output")
#output_dir.mkdir(exist_ok=True)

yearly_data_file = output_dir / "movies_by_year.parquet"
meta_file = output_dir / "meta_movies_by_year.json"

# === Récupération du nombre total de films via analytics ===
#analytics = client.get_analytics()
api_movie_count = analytics.movie_count

# === Lecture du cache s’il existe ===
if meta_file.exists():
    with open(meta_file, "r") as f:
        meta = json.load(f)
    cached_movie_count = meta.get("movie_count", 0)
else:
    cached_movie_count = 0

# === Utilisation du cache ou recalcul ===
if yearly_data_file.exists() and cached_movie_count == api_movie_count:
    print("Chargement des données depuis le cache...")
    df_yearly = pd.read_parquet(yearly_data_file)

else:
    print("Extraction des années depuis l’API...")

    # === Initialisation ===
    year_counter = Counter()
    skip = 0
    limit = 500
    year_pattern = re.compile(r"\((\d{4})\)$")

    while True:
        batch = client.list_movies(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break

        for movie in batch:
            title = movie.get("title", "")
            match = year_pattern.search(title)
            if match:
                year = int(match.group(1))
                year_counter[year] += 1

        skip += limit
        time.sleep(0.5)

    # === Construction du DataFrame ===
    df_yearly = pd.DataFrame(sorted(year_counter.items()), columns=["year", "movie_count"])

    # === Sauvegarde du cache ===
    df_yearly.to_parquet(yearly_data_file, index=False)
    with open(meta_file, "w") as f:
        json.dump({"movie_count": api_movie_count}, f)

# === Affichage avec Plotly ===
fig = px.bar(
    df_yearly,
    x="year",
    y="movie_count",
    title="Nombre total de films par année (basé sur le titre)",
    labels={"year": "Année", "movie_count": "Nombre de films"},
)

fig.update_layout(
    xaxis_title="Année",
    yaxis_title="Nombre de films",
    height=500
)

fig.show()

df_yearly

"""## Top 20 des films par nombre d'évaluations"""

# === Dossiers ===
#output_dir = Path("output")
#output_dir.mkdir(exist_ok=True)

top_movies_file = output_dir / "top_movies_by_ratings.parquet"
meta_file = output_dir / "meta_top_movies.json"

# === Récupération des métriques API ===
#analytics = client.get_analytics()
api_movie_count = analytics.movie_count
api_rating_count = analytics.rating_count

# === Vérification du cache ===
if meta_file.exists():
    with open(meta_file, "r") as f:
        meta = json.load(f)
    cached_movie_count = meta.get("movie_count", 0)
    cached_rating_count = meta.get("rating_count", 0)
else:
    cached_movie_count = 0
    cached_rating_count = 0

# === Utilisation du cache ou recalcul ===
if (
    top_movies_file.exists()
    and cached_movie_count == api_movie_count
    and cached_rating_count == api_rating_count
):
    print("Chargement des données depuis le cache...")
    top_movies_df = pd.read_parquet(top_movies_file)

else:
    print("Récupération des évaluations depuis l’API...")

    # === Initialisation des compteurs ===
    movie_rating_count = defaultdict(int)
    movie_rating_sum = defaultdict(float)

    # === Batching des ratings ===
    limit = 500
    skip = 0

    while True:
        batch = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break

        for rating in batch:
            movie_id = rating["movieId"]
            score = rating["rating"]
            movie_rating_count[movie_id] += 1
            movie_rating_sum[movie_id] += score

        skip += limit
        time.sleep(0.5)

    # === Construction DataFrame des stats ===
    stats = [
        {
            "movieId": movie_id,
            "rating_count": movie_rating_count[movie_id],
            "avg_rating": movie_rating_sum[movie_id] / movie_rating_count[movie_id]
        }
        for movie_id in movie_rating_count
    ]

    stats_df = pd.DataFrame(stats)
    top_movies_df = stats_df.sort_values("rating_count", ascending=False).head(20)

    # === Ajout des titres de films via l’API ===
    movie_titles = {}
    for movie_id in top_movies_df["movieId"]:
        try:
            movie_data = client.get_movie(movie_id)
            movie_titles[movie_id] = movie_data.title
        except Exception as e:
            print(f"Erreur récupération titre movieId {movie_id} : {e}")
            movie_titles[movie_id] = f"Movie {movie_id}"

    top_movies_df["title"] = top_movies_df["movieId"].map(movie_titles)

    # === Sauvegarde dans le cache ===
    top_movies_df.to_parquet(top_movies_file, index=False)
    with open(meta_file, "w") as f:
        json.dump(
            {
                "movie_count": api_movie_count,
                "rating_count": api_rating_count
            },
            f
        )

# === Affichage avec Plotly ===
fig = px.bar(
    top_movies_df.sort_values("rating_count", ascending=True),  # Pour affichage de bas en haut
    x="rating_count",
    y="title",
    color="avg_rating",
    orientation="h",
    title="Top 20 des films par nombre d'évaluations",
    labels={
        "title": "Titre du film",
        "rating_count": "Nombre d'évaluations",
        "avg_rating": "Note moyenne"
    },
    color_continuous_scale="viridis"
)

fig.update_layout(
    yaxis={'categoryorder': 'total ascending'},
    height=700
)

fig.show()

top_movies_df

"""## Comportement des Utilisateurs"""

import os
# Chemins des fichiers
#output_dir = "output"
ratings_path = os.path.join(output_dir, "ratings.parquet")

# Vérifier si le dossier output existe
os.makedirs(output_dir, exist_ok=True)

# Obtenir le nombre total d'évaluations
current_rating_count = analytics.rating_count

# Recharger ou régénérer les données
if os.path.exists(ratings_path):
    ratings_df = pd.read_parquet(ratings_path)
    if ratings_df.shape[0] != current_rating_count:
        print("Les données ont changé, rechargement depuis l’API…")
        ratings_df = []
        limit, skip = 500, 0
        while True:
            batch = client.list_ratings(skip=skip, limit=limit, output_format="pandas")
            if batch.empty:
                break
            ratings_df.append(batch)
            skip += limit
            time.sleep(0.5)
        ratings_df = pd.concat(ratings_df, ignore_index=True)
        ratings_df.to_parquet(ratings_path, index=False)
    else:
        print("Données en cache réutilisées.")
else:
    print("Aucune donnée en cache trouvée, téléchargement initial…")
    ratings_df = []
    limit, skip = 500, 0
    while True:
        batch = client.list_ratings(skip=skip, limit=limit, output_format="pandas")
        if batch.empty:
            break
        ratings_df.append(batch)
        skip += limit
        time.sleep(0.5)
    ratings_df = pd.concat(ratings_df, ignore_index=True)
    ratings_df.to_parquet(ratings_path, index=False)

print(ratings_df.info())
ratings_df.head()

# Graphe 1 : Nombre d’évaluations par utilisateur
ratings_per_user = ratings_df['userId'].value_counts().reset_index()
ratings_per_user.columns = ['userId', 'rating_count']
top_users = ratings_per_user.head(10)

# Bar plot horizontal
fig1 = px.bar(
    top_users,
    x="rating_count",
    y=top_users["userId"].astype(str),  # conversion en string pour éviter une échelle numérique
    orientation="h",
    title="Top 10 des utilisateurs par nombre d’évaluations",
    labels={"userId": "Utilisateur", "rating_count": "Nombre d’évaluations"},
    color="rating_count",
    color_continuous_scale="viridis"
)

# Tri pour avoir le plus gros en haut
fig1.update_layout(
    yaxis={'categoryorder':'total ascending'},
    height=500
)

fig1.show()

ratings_per_user

# Graphe 2 : Distribution des notes données
fig2 = px.histogram(
    ratings_df,
    x="rating",
    nbins=10,
    title="Distribution des notes données",
    labels={"rating": "Note"},
)
fig2.update_layout(bargap=0.1)
fig2.show()

# Graphe 3 : Utilisateurs très positifs vs très critiques
avg_rating_per_user = ratings_df.groupby("userId")["rating"].mean().reset_index()
fig3 = px.histogram(
    avg_rating_per_user,
    x="rating",
    nbins=50,
    title="Répartition des moyennes de notes par utilisateur",
    labels={"rating": "Note moyenne"},
)
fig3.update_layout(bargap=0.1)
fig3.show()

"""## Top tags utilisés par les utilisateurs"""

#output_dir = Path("output")
#output_dir.mkdir(exist_ok=True)

tag_usage_file = output_dir / "user_tag_stats.parquet"
meta_file = output_dir / "meta_users_behavior.json"

# Récupération des métriques d’API pour surveiller les changements :
#analytics = client.get_analytics()
api_rating_count = analytics.rating_count
api_tag_count = analytics.tag_count

if meta_file.exists():
    with open(meta_file, "r") as f:
        meta = json.load(f)
else:
    meta = {}

cached_rating_count = meta.get("rating_count", 0)
cached_tag_count = meta.get("tag_count", 0)


# Tags souvent utilisés par certains utilisateurs

if tag_usage_file.exists() and cached_tag_count == api_tag_count:
    print("Chargement du cache : tags utilisés")
    tag_df = pd.read_parquet(tag_usage_file)
else:
    print("Recalcul : tags utilisés")
    tag_counter = Counter()
    limit = 500
    skip = 0

    while True:
        batch = client.list_tags(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break
        for tag in batch:
            tag_text = tag.get("tag", "")
            tag_counter[tag_text] += 1
        skip += limit
        time.sleep(0.5)

    tag_df = pd.DataFrame(tag_counter.items(), columns=["tag", "count"])
    tag_df = tag_df[tag_df["tag"].str.strip() != ""]
    tag_df = tag_df.sort_values("count", ascending=False).head(20)
    tag_df.to_parquet(tag_usage_file, index=False)

fig4 = px.bar(
    tag_df, x="count", y="tag", orientation="h",
    title="Top tags utilisés par les utilisateurs",
    labels={"count": "Nombre d’utilisations", "tag": "Tag"},
    color="count", color_continuous_scale="viridis"
)
fig4.update_layout(yaxis={'categoryorder': 'total ascending'})
fig4.show()

with open(meta_file, "w") as f:
    json.dump({
        "rating_count": api_rating_count,
        "tag_count": api_tag_count
    }, f)

tag_df

'''
###### Autre méthode (plus longue) pour l'analyse du comportement des Utilisateurs ######
#output_dir = Path("output")
#output_dir.mkdir(exist_ok=True)

rating_file = output_dir / "user_rating_stats.parquet"
score_dist_file = output_dir / "score_distribution.parquet"
user_sentiment_file = output_dir / "user_sentiment.parquet"
tag_usage_file = output_dir / "user_tag_stats.parquet"
meta_file = output_dir / "meta_users_behavior.json"

# Récupération des métriques d’API pour surveiller les changements :
#analytics = client.get_analytics()
api_rating_count = analytics.rating_count
api_tag_count = analytics.tag_count

if meta_file.exists():
    with open(meta_file, "r") as f:
        meta = json.load(f)
else:
    meta = {}

cached_rating_count = meta.get("rating_count", 0)
cached_tag_count = meta.get("tag_count", 0)

# 1. Nombre d’évaluations par utilisateur
if rating_file.exists() and cached_rating_count == api_rating_count:
    print("Chargement du cache : évaluations par utilisateur")
    rating_stats = pd.read_parquet(rating_file)
else:
    print("Recalcul : évaluations par utilisateur")
    user_rating_count = defaultdict(int)
    limit = 500
    skip = 0

    while True:
        batch = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break
        for r in batch:
            user_rating_count[r["userId"]] += 1
        skip += limit
        time.sleep(0.5)

    rating_stats = pd.DataFrame(user_rating_count.items(), columns=["userId", "rating_count"])
    rating_stats.to_parquet(rating_file, index=False)

fig1 = px.histogram(
    rating_stats, x="rating_count", nbins=50,
    title="Nombre d’évaluations par utilisateur",
    labels={"rating_count": "Nombre d’évaluations"}
)
fig1.show()


# 2. Distribution des notes données (histogramme)

if score_dist_file.exists() and cached_rating_count == api_rating_count:
    print("Chargement du cache : distribution des notes")
    scores_df = pd.read_parquet(score_dist_file)
else:
    print("Recalcul : distribution des notes")
    scores = []
    limit = 500
    skip = 0

    while True:
        batch = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break
        for r in batch:
            scores.append(r["rating"])
        skip += limit
        time.sleep(0.5)

    scores_df = pd.DataFrame(scores, columns=["rating"])
    scores_df.to_parquet(score_dist_file, index=False)

fig2 = px.histogram(
    scores_df, x="rating", nbins=10,
    title="Distribution des notes données",
    labels={"rating": "Note"}
)
fig2.show()


# 3. Utilisateurs très positifs vs très critiques
if user_sentiment_file.exists() and cached_rating_count == api_rating_count:
    print("Chargement du cache : score moyen par utilisateur")
    sentiment_df = pd.read_parquet(user_sentiment_file)
else:
    print("Recalcul : score moyen par utilisateur")
    user_ratings = defaultdict(list)
    limit = 500
    skip = 0

    while True:
        batch = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break
        for r in batch:
            user_ratings[r["userId"]].append(r["rating"])
        skip += limit
        time.sleep(0.5)

    sentiment_df = pd.DataFrame([
        {"userId": uid, "avg_rating": sum(ratings) / len(ratings), "count": len(ratings)}
        for uid, ratings in user_ratings.items() if len(ratings) >= 10
    ])
    sentiment_df.to_parquet(user_sentiment_file, index=False)

fig3 = px.histogram(
    sentiment_df, x="avg_rating", nbins=20,
    title="Utilisateurs très positifs vs très critiques",
    labels={"avg_rating": "Note moyenne par utilisateur"}
)
fig3.show()


# 4. Tags souvent utilisés par certains utilisateurs

if tag_usage_file.exists() and cached_tag_count == api_tag_count:
    print("Chargement du cache : tags utilisés")
    tag_df = pd.read_parquet(tag_usage_file)
else:
    print("Recalcul : tags utilisés")
    tag_counter = Counter()
    limit = 500
    skip = 0

    while True:
        batch = client.list_tags(skip=skip, limit=limit, output_format="dict")
        if not batch:
            break
        for tag in batch:
            tag_text = tag.get("tag", "")
            tag_counter[tag_text] += 1
        skip += limit
        time.sleep(0.5)

    tag_df = pd.DataFrame(tag_counter.items(), columns=["tag", "count"])
    tag_df = tag_df[tag_df["tag"].str.strip() != ""]
    tag_df = tag_df.sort_values("count", ascending=False).head(20)
    tag_df.to_parquet(tag_usage_file, index=False)

fig4 = px.bar(
    tag_df, x="count", y="tag", orientation="h",
    title="Top tags utilisés par les utilisateurs",
    labels={"count": "Nombre d’utilisations", "tag": "Tag"},
    color="count", color_continuous_scale="viridis"
)
fig4.update_layout(yaxis={'categoryorder': 'total ascending'})
fig4.show()

with open(meta_file, "w") as f:
    json.dump({
        "rating_count": api_rating_count,
        "tag_count": api_tag_count
    }, f)
'''
0

"""## Insights sur les Tags"""

import pickle

analytics_path = os.path.join(output_dir, "analytics.pkl")
tags_by_genre_path = os.path.join(output_dir, "tags_by_genre.parquet")
tags_good_rating_path = os.path.join(output_dir, "tags_good_rating.parquet")
tags_compare_path = os.path.join(output_dir, "tags_compare.parquet")

# Récupérer les statistiques actuelles de l’API
current_stats = client.get_analytics().__dict__

# Fonction utilitaire pour charger ou recalculer un cache
def use_or_generate(path, current_stats, compute_fn):
    if os.path.exists(path) and os.path.exists(analytics_path):
        with open(analytics_path, "rb") as f:
            saved_stats = pickle.load(f)
        if saved_stats == current_stats:
            return pd.read_parquet(path)

    df = compute_fn()
    df.to_parquet(path, index=False)
    with open(analytics_path, "wb") as f:
        pickle.dump(current_stats, f)
    return df

# -------------------------------
# 1. Tags les plus utilisés par genre
# -------------------------------
def compute_tags_by_genre():
    genre_tag_counter = defaultdict(Counter)

    # Chargement par lots
    skip = 0
    limit = 500
    while True:
        movies = client.list_movies(skip=skip, limit=limit, output_format="dict")
        if not movies:
            break
        movie_dict = {m["movieId"]: m["genres"].split("|") if m["genres"] else [] for m in movies}

        tags = client.list_tags(skip=skip, limit=limit, output_format="dict")
        for tag in tags:
            genres = movie_dict.get(tag["movieId"], [])
            for genre in genres:
                genre_tag_counter[genre][tag["tag"]] += 1

        skip += limit
        time.sleep(0.5)

    records = []
    for genre, tag_counter in genre_tag_counter.items():
        for tag, count in tag_counter.items():
            records.append({"genre": genre, "tag": tag, "count": count})
    df = pd.DataFrame(records)
    df = df.sort_values(["genre", "count"], ascending=[True, False])
    return df

tags_by_genre_df = use_or_generate(tags_by_genre_path, current_stats, compute_tags_by_genre)

# Visualisation : Tags les plus utilisés par genre
# top_tags_by_genre = tags_by_genre_df.groupby("genre").head(3)
# fig1 = px.bar(
#     top_tags_by_genre,
#     x="count",
#     y="tag",
#     color="genre",
#     title="Top Tags les plus utilisés par Genre",
#     orientation="h",
#     facet_col="genre",
#     height=600
# )
# fig1.show()

# Top 3 tags par genre
top_tags_by_genre = tags_by_genre_df.groupby("genre").apply(lambda g: g.nlargest(3, 'count')).reset_index(drop=True)

# Concatène genre + tag pour lisibilité
top_tags_by_genre["tag_label"] = top_tags_by_genre["tag"] + " (" + top_tags_by_genre["genre"] + ")"

fig = px.bar(
    top_tags_by_genre.sort_values("count"),
    x="count",
    y="tag_label",
    color="genre",
    orientation="h",
    title="Top 3 Tags les plus utilisés par Genre",
    labels={"count": "Nombre d'occurrences", "tag_label": "Tag (Genre)"},
    height=800
)
fig.update_layout(yaxis=dict(categoryorder='total ascending'))
fig.show()


# -------------------------------
# 2. Tags les plus fréquents dans les films bien notés (>= 4)
# -------------------------------
def compute_tags_for_good_ratings():
    good_ratings = []
    tags_by_movie = defaultdict(list)

    # Charger les ratings >= 4
    skip = 0
    limit = 500
    while True:
        ratings = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not ratings:
            break
        good_ratings += [r for r in ratings if r["rating"] >= 4]
        skip += limit
        time.sleep(0.5)

    # Associer les tags aux movieId bien notés
    movie_ids = set([r["movieId"] for r in good_ratings])
    skip = 0
    limit = 500
    while True:
        tags = client.list_tags(skip=skip, limit=limit, output_format="dict")
        if not tags:
            break
        for tag in tags:
            if tag["movieId"] in movie_ids:
                tags_by_movie[tag["tag"]].append(tag["movieId"])
        skip += limit
        time.sleep(0.5)

    df = pd.DataFrame([(tag, len(movies)) for tag, movies in tags_by_movie.items()],
                      columns=["tag", "count"])
    df = df.sort_values("count", ascending=False).head(20)
    return df

tags_good_rating_df = use_or_generate(tags_good_rating_path, current_stats, compute_tags_for_good_ratings)

# Visualisation : Tags les plus fréquents dans films bien notés
fig2 = px.bar(
    tags_good_rating_df,
    x="count",
    y="tag",
    orientation="h",
    title="Tags les plus fréquents dans les films bien notés (note ≥ 4)",
    labels={"count": "Nombre d’occurrences", "tag": "Tag"},
    color="count",
    color_continuous_scale="viridis"
)
fig2.update_layout(yaxis={'categoryorder':'total ascending'})
fig2.show()

# -------------------------------
# 3. Comparaison : Tags dans films bien notés vs mal notés
# -------------------------------
def compute_tags_compare():
    tag_counter_good = Counter()
    tag_counter_bad = Counter()

    # Ratings par lots
    skip = 0
    limit = 500
    rating_map = {}
    while True:
        ratings = client.list_ratings(skip=skip, limit=limit, output_format="dict")
        if not ratings:
            break
        for r in ratings:
            rating_map[r["movieId"]] = rating_map.get(r["movieId"], []) + [r["rating"]]
        skip += limit
        time.sleep(0.5)

    # Moyenne par film
    movie_avg_rating = {
        mid: sum(ratings)/len(ratings)
        for mid, ratings in rating_map.items()
    }

    # Tags par lots
    skip = 0
    while True:
        tags = client.list_tags(skip=skip, limit=limit, output_format="dict")
        if not tags:
            break
        for tag in tags:
            avg_rating = movie_avg_rating.get(tag["movieId"])
            if avg_rating is not None:
                if avg_rating >= 4:
                    tag_counter_good[tag["tag"]] += 1
                elif avg_rating < 3:
                    tag_counter_bad[tag["tag"]] += 1
        skip += limit
        time.sleep(0.5)

    tags = set(tag_counter_good.keys()) | set(tag_counter_bad.keys())
    data = []
    for tag in tags:
        data.append({
            "tag": tag,
            "count_good": tag_counter_good.get(tag, 0),
            "count_bad": tag_counter_bad.get(tag, 0)
        })
    df = pd.DataFrame(data)
    df["total"] = df["count_good"] + df["count_bad"]
    df = df[df["total"] > 5].sort_values("total", ascending=False).head(20)
    return df

tags_compare_df = use_or_generate(tags_compare_path, current_stats, compute_tags_compare)

# Visualisation : Comparaison des tags
fig3 = px.bar(
    tags_compare_df.melt(id_vars="tag", value_vars=["count_good", "count_bad"],
                         var_name="Type", value_name="count"),
    x="count",
    y="tag",
    color="Type",
    barmode="group",
    title="Comparaison des Tags : Films bien notés vs mal notés",
    labels={"count": "Nombre d’occurrences", "tag": "Tag"}
)
fig3.update_layout(yaxis={'categoryorder':'total ascending'}, height=600)
fig3.show()

pd.read_parquet(tags_good_rating_path)

tags_good_rating_df

tags_by_genre_df